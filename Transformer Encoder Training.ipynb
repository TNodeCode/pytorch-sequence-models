{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.embedding import *\n",
    "from models.transformer import PyTorchTransformerEncoder\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "lr=1e-4\n",
    "num_layers=3\n",
    "embedding_dim=32\n",
    "batch_size=256\n",
    "max_length=20\n",
    "heads=4\n",
    "dropout=0.1\n",
    "ignore_index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_en = np.load(\"small_vocab_en.npz\")[\"data\"]\n",
    "sequences_fr = np.load(\"small_vocab_fr.npz\")[\"data\"]\n",
    "vocab_size_en = sequences_en.max()\n",
    "vocab_size_fr = sequences_fr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZSequencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class loads data from a npz file\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_seqs_filename: str,\n",
    "        output_seqs_filename: str,\n",
    "        key=\"data\",\n",
    "        max_length=50,\n",
    "        trainingset_size: float = 0.9,\n",
    "        train=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset class\n",
    "        :param filename: The filename of the CSV file\n",
    "        \"\"\"\n",
    "        source_seqs = np.load(input_seqs_filename)[key][:, :max_length]\n",
    "        target_seqs = np.load(output_seqs_filename)[key][:, :max_length]\n",
    "        limit = int(sequences_en.shape[0] * trainingset_size)\n",
    "        self.source_seqs = source_seqs[:limit] if train else source_seqs[limit:]\n",
    "        self.target_seqs = target_seqs[:limit] if train else target_seqs[limit:]\n",
    "        self.vocab_in_size = self.source_seqs.max()+1\n",
    "        self.vocab_out_size = self.target_seqs.max()+1\n",
    "        if self.source_seqs.shape[0] != self.target_seqs.shape[0]:\n",
    "            raise Exception(\"Number of samples of source and target sequences must be equal\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This function returns the total number of items in the dataset.\n",
    "        We are using a pandas data frame in this dataset which has an attribut named shape.\n",
    "        The first dimension of shape is equal to the number of items in the dataset.\n",
    "        :return: The number of rows in the CSV file\n",
    "        \"\"\"\n",
    "        return self.source_seqs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This function returns a single tuple from the dataset.\n",
    "        :param idx: The index of the tuple that should be returned.\n",
    "        :return: Tuple of an x-value and a y-value\n",
    "        \"\"\"\n",
    "        return self.source_seqs[idx], self.target_seqs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NPZSequencesDataset(\"small_vocab_en.npz\", \"small_vocab_fr.npz\", max_length=max_length)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "dataset_test = NPZSequencesDataset(\"small_vocab_en.npz\", \"small_vocab_fr.npz\", max_length=max_length, train=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "input_seqs, target_seqs = next(iter(dataloader_train))\n",
    "input_seqs = input_seqs.to(device)\n",
    "target_seqs = target_seqs.to(torch.long).to(device)\n",
    "input_seqs.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"transformer_temp2.pt\"\n",
    "\n",
    "# Transformer model\n",
    "model = PyTorchTransformerEncoder(\n",
    "    embedding_type=EmbeddingType.POS_LEARNED,\n",
    "    src_vocab_size=dataset_train.vocab_in_size,\n",
    "    trg_vocab_size=dataset_train.vocab_out_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    src_pad_idx=ignore_index,\n",
    "    trg_pad_idx=ignore_index,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss(ignore_index=ignore_index)\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature sequences through the model\n",
    "output = model(input_seqs[:, :-1], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes of the model\n",
    "topv, topi = output.topk(1, dim=2)\n",
    "output.shape, topi.shape, topv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.0\n",
    "for i in range(max_length-1):\n",
    "    _loss = criterion(output[:, i, :], target_seqs[:, i])\n",
    "    if not _loss.isnan():\n",
    "        loss = loss + _loss\n",
    "loss.item() / max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "print_every = 1\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##############################\n",
    "    #    TRANSFORMER TRAINING    #\n",
    "    ############################## \n",
    "    \n",
    "    # Get a batch of training data\n",
    "    for input_seqs, target_seqs in dataloader_train:\n",
    "        # Set gradients of all model parameters to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = torch.tensor(0.0).to(device)\n",
    "        accuracy = 0.0\n",
    "    \n",
    "        input_seqs = input_seqs.to(device)\n",
    "        target_seqs = target_seqs.to(torch.long).to(device)\n",
    "        \n",
    "        # Run the input sequences through the model\n",
    "        output = model(input_seqs[:, :-1], None)\n",
    "\n",
    "        # Iterate over sequence positions to compute the loss\n",
    "        for i in range(max_length-1):\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output[:, i, :].topk(1)\n",
    "            _loss = criterion(output[:, i, :], target_seqs[:, i+1])\n",
    "            if not _loss.isnan():\n",
    "                loss += _loss\n",
    "                mask = target_seqs[:, i+1] != 2\n",
    "                accuracy += float((topi.squeeze()[mask] == target_seqs[mask, i+1]).sum() / (target_seqs[mask].size(0)*(target_seqs[mask].size(1)-2)))\n",
    "\n",
    "        history.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if not epoch % print_every:\n",
    "            _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"LR\", lr, \"ACCURACY\", _accuracy)\n",
    "\n",
    "        ######################\n",
    "        #   WEIGHTS UPDATE   #\n",
    "        ######################\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        accuracy = 0.0\n",
    "\n",
    "        # Update weights of encoder and decoder\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_seqs, target_seqs in dataloader_train:\n",
    "        # Move batch data to the device\n",
    "        input_seqs = input_seqs.to(device)\n",
    "        target_seqs = target_seqs[:, 1:].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_seqs[:, :-1])\n",
    "\n",
    "        # Compute the predicted classes\n",
    "        topv, topi = output.topk(1)\n",
    "\n",
    "        # Iterate over sequence positions to compute the loss\n",
    "        accuracy = 0.0\n",
    "        for i in range(max_length-1):\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output[:, i, :].topk(1)\n",
    "            mask = target_seqs[:, i] != 2\n",
    "            accuracy += float((topi.squeeze()[mask] == target_seqs[mask, i]).sum() / (target_seqs[mask].size(0)*(target_seqs[mask].size(1)-2)))\n",
    "        print(\"ACC\", accuracy)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on the test dataset: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
