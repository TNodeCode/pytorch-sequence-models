{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Datasets Integration Example\n",
    "\n",
    "This notebook demonstrates how to use HuggingFace datasets with the PyTorch sequence models in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.huggingface_dataset import (\n",
    "    HuggingFaceDatasetAdapter,\n",
    "    HuggingFaceSequenceClassificationDataset\n",
    ")\n",
    "from models.transformer import PyTorchTransformerEncoder\n",
    "from models.embedding import EmbeddingType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using HuggingFace Datasets with the Adapter\n",
    "\n",
    "The `HuggingFaceDatasetAdapter` allows you to use any HuggingFace dataset with the models in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a dataset from HuggingFace Hub\n",
    "# Note: This is a conceptual example. You may need to tokenize the data first.\n",
    "# For this example, we'll create a simple synthetic dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create a simple example dataset\n",
    "data = {\n",
    "    'input_ids': [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]],\n",
    "    'labels': [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n",
    "}\n",
    "hf_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Wrap it with our adapter\n",
    "adapted_dataset = HuggingFaceDatasetAdapter(\n",
    "    hf_dataset=hf_dataset,\n",
    "    input_column='input_ids',\n",
    "    target_column='labels'\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(adapted_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Test the dataloader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "    print(f\"Target shape: {targets.shape}\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Custom Text Classification Dataset\n",
    "\n",
    "The `HuggingFaceSequenceClassificationDataset` class allows you to create custom datasets for text classification that are compatible with both PyTorch and HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for demonstration\n",
    "num_samples = 100\n",
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "num_classes = 5\n",
    "\n",
    "# Generate random sequences (in practice, these would be tokenized text)\n",
    "sequences = [np.random.randint(1, vocab_size, size=np.random.randint(10, max_length)).tolist() \n",
    "             for _ in range(num_samples)]\n",
    "labels = np.random.randint(0, num_classes, size=num_samples).tolist()\n",
    "\n",
    "# Create the dataset\n",
    "classification_dataset = HuggingFaceSequenceClassificationDataset(\n",
    "    sequences=sequences,\n",
    "    labels=labels,\n",
    "    max_length=max_length,\n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(classification_dataset)}\")\n",
    "print(f\"Sample item: {classification_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Custom Dataset to HuggingFace Format\n",
    "\n",
    "You can convert your custom dataset to a HuggingFace Dataset object for compatibility with HuggingFace tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "hf_dataset_from_custom = classification_dataset.to_huggingface_dataset()\n",
    "\n",
    "print(f\"HuggingFace Dataset: {hf_dataset_from_custom}\")\n",
    "print(f\"First item: {hf_dataset_from_custom[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the Dataset with a Transformer Encoder Model\n",
    "\n",
    "Now let's use the custom dataset with one of the models from this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set hyperparameters\n",
    "embedding_dim = 64\n",
    "num_layers = 2\n",
    "heads = 4\n",
    "batch_size = 16\n",
    "\n",
    "# Create model\n",
    "model = PyTorchTransformerEncoder(\n",
    "    embedding_type=EmbeddingType.POS_LEARNED,\n",
    "    src_vocab_size=vocab_size,\n",
    "    trg_vocab_size=num_classes,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=0.1,\n",
    "    device=device,\n",
    "    max_length=max_length\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    classification_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_ids)\n",
    "        \n",
    "        print(f\"Input shape: {input_ids.shape}\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example: Loading a Real HuggingFace Dataset\n",
    "\n",
    "Here's how you might use a real dataset from the HuggingFace Hub (commented out as it requires tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a real dataset (uncomment to use)\n",
    "# Note: You'll need to tokenize the text data first\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = load_dataset(\"imdb\", split=\"train[:100]\")\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Tokenize the dataset\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Use the adapter\n",
    "# adapted_dataset = HuggingFaceDatasetAdapter(\n",
    "#     hf_dataset=tokenized_dataset,\n",
    "#     input_column='input_ids',\n",
    "#     target_column='label'\n",
    "# )\n",
    "\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(adapted_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. How to use the `HuggingFaceDatasetAdapter` to wrap HuggingFace datasets for use with models in this repository\n",
    "2. How to create custom text classification datasets using `HuggingFaceSequenceClassificationDataset`\n",
    "3. How to convert custom datasets to HuggingFace format\n",
    "4. How to use these datasets with the Transformer models in the repository\n",
    "\n",
    "The integration allows you to leverage the extensive HuggingFace datasets ecosystem while using the sequence models provided in this repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
