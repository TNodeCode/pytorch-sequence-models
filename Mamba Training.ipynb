{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Model Training Example\n",
    "\n",
    "This notebook demonstrates how to use the Mamba model for sequence classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.embedding import *\n",
    "from models.mamba import Mamba, MambaEncoder\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "lr = 1e-4\n",
    "num_layers = 4\n",
    "embedding_dim = 64\n",
    "batch_size = 32\n",
    "max_length = 20\n",
    "d_state = 16\n",
    "d_conv = 4\n",
    "expand_factor = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Data\n",
    "\n",
    "For testing purposes, we'll create synthetic sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "vocab_size_src = 100\n",
    "vocab_size_trg = 50\n",
    "num_samples = 1000\n",
    "\n",
    "# Random sequences for testing\n",
    "sequences_src = np.random.randint(0, vocab_size_src, size=(num_samples, max_length))\n",
    "sequences_trg = np.random.randint(0, vocab_size_trg, size=(num_samples, max_length))\n",
    "\n",
    "print(f\"Source sequences shape: {sequences_src.shape}\")\n",
    "print(f\"Target sequences shape: {sequences_trg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, src_seqs, trg_seqs):\n",
    "        self.src_seqs = src_seqs\n",
    "        self.trg_seqs = trg_seqs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_seqs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.src_seqs[idx], dtype=torch.long),\n",
    "            torch.tensor(self.trg_seqs[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "dataset = SyntheticDataset(sequences_src, sequences_trg)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Mamba Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mamba model\n",
    "model = MambaEncoder(\n",
    "    embedding_type=EmbeddingType.POS_LEARNED,\n",
    "    src_vocab_size=vocab_size_src,\n",
    "    trg_vocab_size=vocab_size_trg,\n",
    "    embedding_dim=embedding_dim,\n",
    "    d_state=d_state,\n",
    "    d_conv=d_conv,\n",
    "    expand_factor=expand_factor,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    max_length=max_length\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single batch\n",
    "src_batch, trg_batch = next(iter(dataloader))\n",
    "src_batch = src_batch.to(device)\n",
    "trg_batch = trg_batch.to(device)\n",
    "\n",
    "print(f\"Input shape: {src_batch.shape}\")\n",
    "print(f\"Target shape: {trg_batch.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(src_batch)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: ({batch_size}, {max_length}, {vocab_size_trg})\")\n",
    "print(\"\\nForward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output = output.reshape(-1, vocab_size_trg)\n",
    "        trg = trg.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Mamba Model Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
