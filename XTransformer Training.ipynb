{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataset.npz_dataset import NPZSequencesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from models.embedding import *\n",
    "from models.xlstm import XTransformer\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=50\n",
    "lr=1e-4\n",
    "num_layers=4\n",
    "factor=2\n",
    "embedding_dim=64\n",
    "batch_size=1024\n",
    "max_length=20\n",
    "heads=4\n",
    "dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_en = np.load(\"data/small_vocab_en.npz\")[\"data\"]\n",
    "sequences_fr = np.load(\"data/small_vocab_fr.npz\")[\"data\"]\n",
    "vocab_size_en = sequences_en.max()\n",
    "vocab_size_fr = sequences_fr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NPZSequencesDataset(\"data/small_vocab_en.npz\", \"data/small_vocab_fr.npz\", split='train', max_length=max_length)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "dataset_val = NPZSequencesDataset(\"data/small_vocab_en.npz\", \"data/small_vocab_fr.npz\", split='val', max_length=max_length)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "dataset_test = NPZSequencesDataset(\"data/small_vocab_en.npz\", \"data/small_vocab_fr.npz\", split='test', max_length=max_length)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "input_seqs, target_seqs = next(iter(dataloader_train))\n",
    "input_seqs = input_seqs.to(device)\n",
    "target_seqs = target_seqs.to(torch.long).to(device)\n",
    "input_seqs.shape, target_seqs.shape, dataset_train.vocab_in_size, dataset_train.vocab_out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"transformer_temp2.pt\"\n",
    "\n",
    "# Transformer model\n",
    "model = XTransformer(\n",
    "    embedding_type=EmbeddingType.POS_LEARNED,\n",
    "    src_vocab_size=dataset_train.vocab_in_size,\n",
    "    tgt_vocab_size=dataset_train.vocab_out_size,\n",
    "    config_layers='m',\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_length=max_length-1,\n",
    "    num_layers=num_layers,\n",
    "    factor=factor,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss(ignore_index=2)\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature sequences through the model\n",
    "output = model(input_seqs[:, :-1], target_seqs[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes of the model\n",
    "topv, topi = output.topk(1, dim=2)\n",
    "output.shape, topi.shape, topv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.0\n",
    "for i in range(max_length-1):\n",
    "    _loss = criterion(output[:, i, :], target_seqs[:, i])\n",
    "    if not _loss.isnan():\n",
    "        loss = loss + _loss\n",
    "loss.item() / max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "print_every = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##############################\n",
    "    #    TRANSFORMER TRAINING    #\n",
    "    ############################## \n",
    "    \n",
    "    # Get a batch of training data\n",
    "    for b, (input_seqs, target_seqs) in enumerate(dataloader_train):\n",
    "        # Set gradients of all model parameters to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = torch.tensor(0.0).to(device)\n",
    "        accuracy = 0.0\n",
    "    \n",
    "        input_seqs = input_seqs.to(device)\n",
    "        target_seqs = target_seqs.to(torch.long).to(device)\n",
    "        \n",
    "        # Run the input sequences through the model\n",
    "        output = model(input_seqs[:, :-1], target_seqs[:, :-1])\n",
    "\n",
    "        # Iterate over sequence positions to compute the loss\n",
    "        for i in range(max_length-1):\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output[:, i, :].topk(1)\n",
    "            _loss = criterion(output[:, i, :], target_seqs[:, i+1])\n",
    "            if not _loss.isnan():\n",
    "                loss += _loss\n",
    "                mask = target_seqs[:, i+1] != 2\n",
    "                accuracy += float((topi.squeeze()[mask] == target_seqs[mask, i+1]).sum() / (target_seqs[mask].size(0)*(target_seqs[mask].size(1)-2)))\n",
    "\n",
    "        history.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if not epoch % print_every:\n",
    "            _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"LOSS after epoch {epoch} Batch [{b+1}/{len(dataloader_train)}]\", loss.item() / (target_seqs.size(1)), \"LR\", lr, \"ACCURACY\", _accuracy)\n",
    "\n",
    "        ######################\n",
    "        #   WEIGHTS UPDATE   #\n",
    "        ######################\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        accuracy = 0.0\n",
    "\n",
    "        # Update weights of encoder and decoder\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "batch_accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_seqs, target_seqs in dataloader_test:\n",
    "        # Move batch data to the device\n",
    "        input_seqs = input_seqs.to(device)\n",
    "        target_seqs = target_seqs[:, 1:].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_seqs[:, :-1])\n",
    "\n",
    "        # Compute the predicted classes\n",
    "        topv, topi = output.topk(1)\n",
    "\n",
    "        # Iterate over sequence positions to compute the loss\n",
    "        accuracy = 0.0\n",
    "        for i in range(max_length-1):\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output[:, i, :].topk(1)\n",
    "            mask = target_seqs[:, i] != 2\n",
    "            accuracy += float((topi.squeeze()[mask] == target_seqs[mask, i]).sum() / (target_seqs[mask].size(0)*(target_seqs[mask].size(1)-2)))\n",
    "        batch_accuracies.append(accuracy)\n",
    "        print(\"ACC\", accuracy)\n",
    "\n",
    "# Compute the accuracy\n",
    "mean_accuracy = np.array(batch_accuracies).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on the test dataset: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
