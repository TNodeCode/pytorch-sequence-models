{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space Model Training Example\n",
    "\n",
    "This notebook demonstrates how to use the State Space Model for sequence classification tasks.\n",
    "\n",
    "State Space Models (SSMs) are a powerful class of models for sequence modeling that offer an alternative to Transformers and RNNs. They are based on continuous-time state space equations discretized for sequence processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from models.state_space import StateSpaceModel, StateSpaceEncoder\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Device Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Synthetic Dataset\n",
    "\n",
    "For this example, we'll create a simple binary classification task: classify sequences as having more 1s than 0s or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(num_samples=1000, seq_length=20, vocab_size=10, num_classes=2):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset for sequence classification.\n",
    "    Task: Classify sequences based on whether they contain more odd or even numbers.\n",
    "    \"\"\"\n",
    "    # Generate random sequences\n",
    "    sequences = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
    "    \n",
    "    # Create labels: 1 if more odd numbers, 0 if more even numbers\n",
    "    labels = torch.zeros(num_samples, dtype=torch.long)\n",
    "    for i in range(num_samples):\n",
    "        odd_count = (sequences[i] % 2).sum().item()\n",
    "        labels[i] = 1 if odd_count > seq_length / 2 else 0\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Create train and test datasets\n",
    "train_sequences, train_labels = create_synthetic_dataset(num_samples=800, seq_length=20, vocab_size=10)\n",
    "test_sequences, test_labels = create_synthetic_dataset(num_samples=200, seq_length=20, vocab_size=10)\n",
    "\n",
    "print(f\"Train dataset: {train_sequences.shape}, {train_labels.shape}\")\n",
    "print(f\"Test dataset: {test_sequences.shape}, {test_labels.shape}\")\n",
    "print(f\"\\nExample sequence: {train_sequences[0]}\")\n",
    "print(f\"Example label: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_sequences, train_labels)\n",
    "test_dataset = TensorDataset(test_sequences, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize the State Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = 10\n",
    "num_classes = 2\n",
    "embedding_dim = 64\n",
    "d_state = 32\n",
    "num_layers = 4\n",
    "max_length = 20\n",
    "dropout = 0.1\n",
    "pooling = 'mean'  # Options: 'mean', 'max', 'last', 'cls'\n",
    "\n",
    "# Create the model\n",
    "model = StateSpaceModel(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=embedding_dim,\n",
    "    d_state=d_state,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    max_length=max_length,\n",
    "    pooling=pooling,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params:,} trainable parameters\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler (optional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (sequences, labels) in enumerate(dataloader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in dataloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    print()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax1.plot(test_losses, label='Test Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
    "ax2.plot(test_accs, label='Test Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Model on Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few test samples\n",
    "num_samples = 5\n",
    "sample_sequences = test_sequences[:num_samples].to(device)\n",
    "sample_labels = test_labels[:num_samples]\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_sequences)\n",
    "    _, predictions = outputs.max(1)\n",
    "\n",
    "# Display results\n",
    "print(\"Sample Predictions:\\n\")\n",
    "for i in range(num_samples):\n",
    "    seq = sample_sequences[i].cpu().numpy()\n",
    "    true_label = sample_labels[i].item()\n",
    "    pred_label = predictions[i].item()\n",
    "    \n",
    "    odd_count = sum(1 for x in seq if x % 2 == 1)\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Sequence: {seq}\")\n",
    "    print(f\"  Odd count: {odd_count}/{len(seq)}\")\n",
    "    print(f\"  True label: {true_label} ({'Odd majority' if true_label == 1 else 'Even majority'})\")\n",
    "    print(f\"  Predicted: {pred_label} ({'Odd majority' if pred_label == 1 else 'Even majority'})\")\n",
    "    print(f\"  Correct: {'✓' if true_label == pred_label else '✗'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Configuration Options\n",
    "\n",
    "The State Space Model supports various configuration options:\n",
    "\n",
    "### Pooling Strategies\n",
    "- `'mean'`: Average pooling over the sequence\n",
    "- `'max'`: Max pooling over the sequence\n",
    "- `'last'`: Use the last token's representation\n",
    "- `'cls'`: Use the first token (CLS token style)\n",
    "\n",
    "### Key Hyperparameters\n",
    "- `embedding_dim`: Dimension of token embeddings and hidden states\n",
    "- `d_state`: Dimension of the internal state space (larger = more expressive but slower)\n",
    "- `num_layers`: Number of State Space blocks to stack\n",
    "- `dropout`: Dropout probability for regularization\n",
    "\n",
    "### Example: Try Different Pooling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different pooling strategies\n",
    "pooling_methods = ['mean', 'max', 'last', 'cls']\n",
    "results = {}\n",
    "\n",
    "for pooling in pooling_methods:\n",
    "    # Create model with specific pooling\n",
    "    test_model = StateSpaceModel(\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        embedding_dim=32,  # Smaller for faster comparison\n",
    "        d_state=16,\n",
    "        num_layers=2,\n",
    "        pooling=pooling,\n",
    "        max_length=max_length,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    \n",
    "    # Quick evaluation on test set\n",
    "    _, acc = evaluate(test_model, test_loader, criterion, device)\n",
    "    results[pooling] = acc\n",
    "    print(f\"Pooling '{pooling}': {acc:.2f}% accuracy (untrained)\")\n",
    "\n",
    "print(\"\\nNote: These are untrained models, so accuracies should be near random (50%).\")\n",
    "print(\"After training, different pooling methods may perform differently depending on the task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "1. Create a State Space Model for sequence classification\n",
    "2. Train it on a synthetic dataset\n",
    "3. Evaluate its performance\n",
    "4. Make predictions on new sequences\n",
    "\n",
    "State Space Models offer an efficient alternative to Transformers and RNNs for sequence modeling tasks, with the following advantages:\n",
    "- **Efficiency**: Linear-time complexity compared to quadratic for Transformers\n",
    "- **Long sequences**: Better at handling long-range dependencies than RNNs\n",
    "- **Simplicity**: Simpler architecture than attention mechanisms\n",
    "\n",
    "You can adapt this notebook to work with real datasets by:\n",
    "1. Loading your own data (e.g., from HuggingFace Datasets)\n",
    "2. Adjusting the model hyperparameters\n",
    "3. Using appropriate preprocessing and tokenization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
